{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Visual perspective taking in humans and machines   Drew Linsley*<sup>1</sup>, Peisen Zhou*<sup>1</sup>, Alekh Karkada*<sup>1</sup>, Akash Nagaraj<sup>1</sup>,  Gaurav Gaonkar<sup>1</sup>, Francis E Lewis<sup>1</sup>, Zygmunt Pizlo<sup>2</sup>, Thomas Serre<sup>1</sup> <sup>1</sup>Carney Institute for Brain Science, Brown University, Providence, RI 02912  <sup>2</sup>Department of Cognitive Sciences, University of California-Irvine, Irvine, CA.   drew_linsley@brown.edu  <p> Read the official paper \u00bb Data   \u00b7   Github </p> <p></p>"},{"location":"#abstract","title":"Abstract","text":"<p>Visual perspective taking (VPT), the ability to accurately perceive and reason about the perspectives of others, is an essential feature of human intelligence.  VPT is a byproduct of capabilities for analyzing 3-Dimensional (3D) scenes, which develop over the first decade of life.  Deep neural networks (DNNs) may be a good candidate for modeling VPT and  its computational demands in light of a growing number of reports indicating that DNNs gain the ability to analyze 3D scenes after training on large static-image datasets.  Here, we investigated this possibility by developing the 3D perception challenge (3D-PC) for comparing 3D perceptual capabilities in humans and DNNs.  We tested over 30 human participants and 'linearly probed' or text-prompted over 300 DNNs on several 3D-analysis tasks posed within natural scene images:  (i.) a simple test of object depth order, (ii.) a basic VPT task (VPT-basic), and (iii.) a more challenging version of VPT designed to limit the effectiveness of 'shortcut' visual strategies.  Nearly all of the DNNs approached or exceeded human accuracy in analyzing object depth order, and surprisingly, their accuracy on this task correlated with their object recognition performance.  In contrast, there was an extraordinary gap between DNNs and humans on VPT-basic: humans were nearly perfect, whereas most DNNs were near chance.  Fine-tuning DNNs on VPT-basic brought them close to human performance, but they, unlike humans, dropped back to chance when tested on VPT-perturbation.  Our challenge demonstrates that the training routines and architectures of today's DNN are well-suited for learning basic 3D properties of scenes and objects,  but not for reasoning about these properties in the way that humans rely on for their everyday lives. We release our 3D-PC datasets and code to help bridge this gap in 3D perception between humans and machines.</p>"},{"location":"#citation","title":"\ud83d\uddde\ufe0f Citation","text":"<p>If you use or build on our work as part of your workflow in a scientific publication, please consider citing the official paper:</p> <pre><code>@misc{linsley20243dpc,\n      title={The 3D-PC: a benchmark for visual perspective taking in humans and machines}, \n      author={Drew Linsley and Peisen Zhou and Alekh Karkada Ashok and Akash Nagaraj and Gaurav Gaonkar and Francis E Lewis and Zygmunt Pizlo and Thomas Serre},\n      year={2024},\n      eprint={2406.04138},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>The package is released under  CC-BY-4.0 license.</p>"},{"location":"benchmark/","title":"Benchmark","text":""},{"location":"benchmark/#model-evaluation-and-results","title":"Model Evaluation and Results","text":"<p>We evaluated 327 different DNNs on 3D-PC by both linear probing and fine-tuning models on VPT-basic and depth order tasks.</p>"},{"location":"benchmark/#linear-probe-results","title":"Linear Probe Results","text":"<p>DNN performance on the depth order and VPT-basic tasks in the 3D-PC after linear probing or prompting. (A, B) DNNs are significantly more accurate at depth order than VPT-basic. Human confidence intervals are S.E.M. and ***: p &lt; 0.001. (C, D) DNN accuracy for depth order and VPT-basic strongly correlates with object classification accuracy on ImageNet. Dashed lines are the mean of label-permuted human noise floors.</p>"},{"location":"benchmark/#fine-tune-results","title":"Fine-tune Results","text":"<p>DNN performance on the depth order and VPT-basic tasks in the 3D-PC after fine- tuning. (A) Fine-tuning makes DNNs far better than humans at the depth order task and improves the performance of several DNNs to be at or beyond human accuracy on VPT-basic. (B) Even after fine-tuning, there is still a significant difference in model performance on depth order and VPT-basic tasks, p &lt; 0.001 . (C, D) DNN accuracy on both tasks after fine-tuning correlates with ImageNet object classification accuracy. Human confidence intervals are S.E.M. and ***: p &lt; 0.001. Dashed lines are the mean of label-permuted human noise floors.</p>"},{"location":"benchmark/#timm-evaluation","title":"TIMM Evaluation","text":"<p>To linear probe a timm model <pre><code>python run_linear_probe.py --task &lt;task&gt; --data_dir &lt;data_folder&gt;/&lt;task&gt;/ --model_name &lt;model_name&gt;\n</code></pre></p> <p>To fine-tune a timm model <pre><code>python run_finetune.py --task &lt;task&gt; --data_dir &lt;data_folder&gt;/&lt;task&gt;/ --model_name &lt;model_name&gt;\n</code></pre></p> <p><code>data_folder</code>: Root directory for the dataset</p> <p><code>task</code>: Either <code>perspective</code> or <code>depth</code></p> <p><code>model_name</code>: TIMM model name</p>"},{"location":"data/","title":"Data","text":""},{"location":"data/#data-access","title":"Data Access","text":""},{"location":"data/#hugging-face","title":"Hugging Face","text":"<p>We host data for all three tasks: VPT-basic, VPT-strategy, and depth order on Hugging Face.</p> <p>https://huggingface.co/datasets/pzhou10/3D-PC <pre><code>from datasets import load_dataset\n# config_name: one of [\"vpt-basic\", \"vpt-strategy\", \"depth\"]\ndataset = load_dataset(\"pzhou10/3D-PC\", \"vpt-basic\")\n</code></pre></p>"},{"location":"data/#download-the-full-dataset","title":"Download the full dataset","text":"<p>We also release the complete 3D-PC dataset along with data splits for training and testing.</p> <p>https://connectomics.clps.brown.edu/tf_records/VPT/</p>"},{"location":"data/#dataset-content","title":"Dataset Content","text":"<p><code>train</code> contains all training images organized by categories.  <pre><code>train\n|\n|_&lt;category&gt;\n|  |_&lt;object&gt;\n|    |_&lt;setting&gt;\n|      |_&lt;*.png&gt;\n</code></pre> The corresponding labels are <code>train_perspective.csv</code> and <code>depth_perspective.csv</code>. We also provide <code>train_perspective_balanced.csv</code> and <code>depth_perspective_balanced.csv</code>, where the numbers of positive and negative samples are equal.</p> <p><code>perspective</code> and <code>depth</code> contain all data splits for 'VPT' and 'depth order' tasks. <pre><code>perspective/depth\n|\n|_&lt;split&gt;\n|  |_&lt;category&gt; 0/1\n|    |_&lt;*.png&gt;\n</code></pre></p>"}]}